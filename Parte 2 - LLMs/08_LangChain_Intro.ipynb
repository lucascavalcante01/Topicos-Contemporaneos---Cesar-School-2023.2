{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SDqtByR4hDlx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55f67a59-ed41-4bdc-dc0e-ee028d16efe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwQj2_HFhDly"
      },
      "source": [
        "## Modelos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai"
      ],
      "metadata": {
        "id": "z4Z2pbTCiAvr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be92ab6e-cd96-44cd-ecbd-5889d49b92e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.1.22-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain-core<0.3.0,>=0.2.33 (from langchain_openai)\n",
            "  Downloading langchain_core-0.2.34-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting openai<2.0.0,>=1.40.0 (from langchain_openai)\n",
            "  Downloading openai-1.42.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.33->langchain_openai) (6.0.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.33->langchain_openai)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.75 (from langchain-core<0.3.0,>=0.2.33->langchain_openai)\n",
            "  Downloading langsmith-0.1.101-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.33->langchain_openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.33->langchain_openai) (2.8.2)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core<0.3.0,>=0.2.33->langchain_openai)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.33->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.40.0->langchain_openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain_openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.66.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.33->langchain_openai)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.33->langchain_openai)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m739.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.33->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.33->langchain_openai) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.7)\n",
            "Downloading langchain_openai-0.1.22-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.34-py3-none-any.whl (393 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m393.9/393.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.42.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.1.101-py3-none-any.whl (148 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m148.9/148.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, jiter, h11, tiktoken, jsonpatch, httpcore, httpx, openai, langsmith, langchain-core, langchain_openai\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jiter-0.5.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.2.34 langchain_openai-0.1.22 langsmith-0.1.101 openai-1.42.0 orjson-3.10.7 tenacity-8.5.0 tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yl8fPDG3hDly"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CE7homkAhDlz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c869c32-d915-4d6b-9b57-181fd3684441"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='OlÃ¡! Como posso ajudar vocÃª hoje?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 8, 'total_tokens': 16}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None} id='run-6384bc4d-b11c-47c6-abc5-490a84848775-0' usage_metadata={'input_tokens': 8, 'output_tokens': 8, 'total_tokens': 16}\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(\"OlÃ¡\")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6wOFwJQhDlz"
      },
      "source": [
        "## Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hll4izEHhDlz"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gl3SQgShDlz"
      },
      "source": [
        "### Templates Simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "H8quEcfLhDlz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9672c91c-b607-4ee4-a56a-179f94386561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[HumanMessage(content='Traduza o seguinte texto para portuguÃªs: Artificial Intelligence is the future!')]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(\"Traduza o seguinte texto para portuguÃªs: {texto}\")\n",
        "\n",
        "prompt = prompt_template.invoke({\"texto\": \"Artificial Intelligence is the future!\"})\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zsH_Pg_XhDl0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db6304e7-dfe9-4caf-fc43-afe80ae743ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A inteligÃªncia artificial Ã© o futuro!\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0R-k-WlhDl0"
      },
      "source": [
        "### Templates de Mensagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "I_qULd8BhDl0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a845f276-67bb-48a6-a5ba-0464ec84e4d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='OlÃ¡, como vocÃª estÃ¡?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 36, 'total_tokens': 42}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None} id='run-1da0c478-4b1c-4321-bcca-edb82fccf946-0' usage_metadata={'input_tokens': 36, 'output_tokens': 6, 'total_tokens': 42}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"VocÃª Ã© um tradutor de inglÃªs para portuguÃªs. Traduza as mensagens que forem enviadas.\"),\n",
        "    HumanMessage(content=\"Hello, how are you?\"),\n",
        "]\n",
        "\n",
        "# messages = [\n",
        "#     (\"system\", \"VocÃª Ã© um tradutor de inglÃªs para portuguÃªs. Traduza as mensagens que forem enviadas.\"),\n",
        "#     (\"human\", \"Hello, how are you?\"),\n",
        "# ]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AFQhLvKchDl0"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"VocÃª Ã© um tradutor de {lingua_origem} para {lingua_destino}. Traduza as mensagens que forem enviadas.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4kcaihnIhDl0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98a0ca5d-6ea7-490f-e894-20f4406f53e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='VocÃª Ã© um tradutor de inglÃªs para portuguÃªs. Traduza as mensagens que forem enviadas.'), HumanMessage(content='Hello, how are you?')]\n"
          ]
        }
      ],
      "source": [
        "prompt = prompt_template.invoke({\n",
        "    \"lingua_origem\": \"inglÃªs\",\n",
        "    \"lingua_destino\": \"portuguÃªs\",\n",
        "    \"texto\": \"Hello, how are you?\"\n",
        "})\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6MzBIky6hDl1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89669bce-4d9e-4d11-827f-57ab7a5a25ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OlÃ¡, como vocÃª estÃ¡?\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERC1DOoxhDl1"
      },
      "source": [
        "### Parsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sSZYYggshDl1"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2qP_x2vohDl1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bcb7d20-c121-47e1-ad79-af73264881c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OlÃ¡, como vocÃª estÃ¡?\n"
          ]
        }
      ],
      "source": [
        "output = parser.invoke(response)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKHE3d8GhDl1"
      },
      "source": [
        "## Encadeamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HTXpqdlzhDl1"
      },
      "outputs": [],
      "source": [
        "chain = prompt_template | llm | parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KtT06OLDhDl1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32672a18-156e-40e4-872a-e1379316cc3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Â¡Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "response = chain.invoke({\n",
        "    \"lingua_origem\": \"inglÃªs\",\n",
        "    \"lingua_destino\": \"espanhol\",\n",
        "    \"texto\": \"As praias de Recife tem tubarÃµes!\"\n",
        "})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bLkZkF1BhDl1"
      },
      "outputs": [],
      "source": [
        "def translate(texto, lingua_origem, lingua_destino):\n",
        "    response = chain.invoke({\n",
        "        \"lingua_origem\": lingua_origem,\n",
        "        \"lingua_destino\": lingua_destino,\n",
        "        \"texto\": texto\n",
        "    })\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZfFCKgdmhDl1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b2ab27a-7492-42c1-f684-c6706cf1b29c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Â¡Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "output = translate(\"The beaches of Recife have sharks!\", \"inglÃªs\", \"espanhol\")\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kbx2ao8hDl2"
      },
      "source": [
        "## ExercÃ­cios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyUxaMkChDl2"
      },
      "source": [
        "### ExercÃ­cio 1\n",
        "Crie uma `chain` que a partir de um tÃ³pico informado pelo usuÃ¡rio, crie uma piada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "37gTo2_7hDl2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a2c0507f-0688-4761-f771-b21aeca6eeb0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Por que o livro de matemÃ¡tica se suicidou?\\n\\nPorque tinha muitos problemas! ğŸ˜„'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_template('FaÃ§a uma piada sobre {topico}')\n",
        "\n",
        "chain = prompt_template | llm | parser\n",
        "\n",
        "chain.invoke({'topico': 'matemÃ¡tica'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqfqBDd1hDl2"
      },
      "source": [
        "### ExercÃ­cio 2\n",
        "Crie uma `chain` que classifique o sentimento de um texto de entrada em positivo, neutro ou negativo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "scofaVnehDl2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ddb0aac1-fb98-4de8-c3ab-91960fc5112d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'O sentimento do texto Ã© positivo.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_template('Classifique em sentimento positivo, neutro ou negativo o seguinte texto: {texto}')\n",
        "\n",
        "chain = prompt_template | llm | parser\n",
        "\n",
        "chain.invoke({'texto': 'A comida daquele restaurante Ã© sensacional.'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZQmRjWdhDl2"
      },
      "source": [
        "### ExercÃ­cio 3\n",
        "Crie uma `chain` que gere o cÃ³digo de uma funÃ§Ã£o Python de acordo com a descriÃ§Ã£o do usuÃ¡rio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Q8g7iW_zhDl2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a8ea06b-2e97-4a8f-da78-586ecbed0e3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aqui estÃ¡ o cÃ³digo Python para uma funÃ§Ã£o que retorna a mÃ©dia entre dois nÃºmeros:\n",
            "\n",
            "```python\n",
            "def media_dois_numeros(num1, num2):\n",
            "    return (num1 + num2) / 2\n",
            "```\n",
            "\n",
            "VocÃª pode usar essa funÃ§Ã£o passando dois nÃºmeros como argumentos para obter a mÃ©dia. Por exemplo:\n",
            "\n",
            "```python\n",
            "resultado = media_dois_numeros(10, 20)\n",
            "print(resultado)  # SaÃ­da: 15.0\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "      (\"system\", \"VocÃª Ã© um programador especialista em python e converte descriÃ§Ãµes de funÃ§Ãµes em cÃ³digo python. Converta em cÃ³digo python as descriÃ§Ãµes recebidas.\"),\n",
        "      (\"user\", \"{texto}\"),\n",
        "      ]\n",
        "\n",
        ")\n",
        "\n",
        "chain = prompt_template | llm | parser\n",
        "\n",
        "resultado = chain.invoke({\"texto\": \"escreva o cÃ³digo python de uma funÃ§Ã£o que retorne a mÃ©dia entre dois nÃºmeros\"})\n",
        "\n",
        "print(resultado)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueE0NpeVhDl2"
      },
      "source": [
        "### ExercÃ­cio 4\n",
        "Crie uma `chain` que explique de forma simplificada um tÃ³pico geral fornecido pelo usuÃ¡rio e, em seguida, traduza a explicaÃ§Ã£o para inglÃªs. Utilize dois templates encadeados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hRybUhYWhDl2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "d763d639-03af-4228-9693-998b710a5f26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'**ExplicaÃ§Ã£o Simplificada:**\\n\\nA exploraÃ§Ã£o espacial Ã© o processo de investigar o espaÃ§o alÃ©m da Terra. Isso Ã© feito por meio de naves espaciais, sondas, telescÃ³pios e outros equipamentos que ajudam os cientistas a entender mais sobre o universo, planetas, estrelas e galÃ¡xias. A exploraÃ§Ã£o pode incluir missÃµes tripuladas, como as que levam astronautas Ã  Lua ou Ã  EstaÃ§Ã£o Espacial Internacional, e missÃµes nÃ£o tripuladas, que enviam robÃ´s e sondas para estudar outros planetas, como Marte ou JÃºpiter. O objetivo Ã© descobrir mais sobre como o universo funciona, a origem da vida e se existe vida em outros lugares.\\n\\n**TraduÃ§Ã£o:**\\n\\n**Simplified Explanation:**\\n\\nSpace exploration is the process of investigating space beyond Earth. This is done through spacecraft, probes, telescopes, and other equipment that help scientists learn more about the universe, planets, stars, and galaxies. Exploration can include crewed missions, like those that take astronauts to the Moon or the International Space Station, and uncrewed missions that send robots and probes to study other planets, like Mars or Jupiter. The goal is to discover more about how the universe works, the origin of life, and whether there is life elsewhere.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_template('Explique de forma simplificada e depois traduza sua explicaÃ§Ã£o do seguinte tÃ³picp: {topico}')\n",
        "\n",
        "chain = prompt_template | llm | parser\n",
        "\n",
        "chain.invoke({'topico': 'ExploraÃ§Ã£o espacial'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isr0EB-AhDl2"
      },
      "source": [
        "### ExercÃ­cio 5 - Desafio\n",
        "Crie uma `chain` que responda perguntas sobre o CESAR School."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sL6jzNXHhDl3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f629524d-3bfb-4c82-af0f-b2938fcf84ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Cesar School oferece uma variedade de cursos nas Ã¡reas de tecnologia e inovaÃ§Ã£o. Alguns dos cursos disponÃ­veis incluem:\n",
            "\n",
            "1. **MBA em GestÃ£o de Produtos** - Focado em desenvolvimento e gestÃ£o de produtos digitais.\n",
            "2. **MBA em GestÃ£o de Tecnologia da InformaÃ§Ã£o** - Voltado para a administraÃ§Ã£o e gestÃ£o de TI nas organizaÃ§Ãµes.\n",
            "3. **Cursos de FormaÃ§Ã£o em Desenvolvimento de Software** - Incluindo programaÃ§Ã£o, desenvolvimento web e mobile.\n",
            "4. **Cursos de Design** - Como Design de InteraÃ§Ã£o e Design de ExperiÃªncia do UsuÃ¡rio (UX).\n",
            "5. **Data Science e Analytics** - Cursos que abordam anÃ¡lise de dados e tÃ©cnicas de ciÃªncia de dados.\n",
            "6. **Cursos de Empreendedorismo e InovaÃ§Ã£o** - Focados em capacitar empreendedores e fomentar a inovaÃ§Ã£o.\n",
            "\n",
            "Para informaÃ§Ãµes mais detalhadas sobre cada curso, incluindo datas e prÃ©-requisitos, recomendo visitar o site oficial da Cesar School.\n"
          ]
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "      (\"system\", \"VocÃª Ã© um funcionÃ¡rio responsÃ¡vel por fornecer informaÃ§Ãµes sobre o Cesar School. Responda as perguntas a respeito da Cesar School recebidas.\"),\n",
        "      (\"user\", \"{texto}\"),\n",
        "      ]\n",
        "\n",
        ")\n",
        "\n",
        "chain = prompt_template | llm | parser\n",
        "\n",
        "resultado = chain.invoke({\"texto\": \"Quais sÃ£o os cursos disponÃ­veis?\"})\n",
        "\n",
        "print(resultado)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}