{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q-cfOXifOvDt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6wHsyYJmOvDu"
      },
      "outputs": [],
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "d_ff = 2048\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JIE0qHy_OvDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e25727af-7a06-4b6f-f7ba-12856467de4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50, 32, 512])\n"
          ]
        }
      ],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "# Exemplo\n",
        "max_len = 100\n",
        "pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "# (sequence_length, batch_size, d_model)\n",
        "input_tensor = torch.randn(50, batch_size, d_model)\n",
        "output_tensor = pos_encoding(input_tensor)\n",
        "\n",
        "print(output_tensor.shape)  # Output shape: (sequence_length, batch_size, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fVuLnyzLOvDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2bca66f-878f-43ba-c83f-85ced707c7e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 50, 512])\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        # Verifica se o número de dimensões do modelo é divisível pelo número de cabeças\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        # Número de dimensões por cabeça\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Inicializa as camadas lineares para Q, K e V\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        # Calcula os scores fazendo o produto escalar entre Q e K e dividindo pela raiz quadrada de d_k\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Se a máscara for fornecida, aplica a máscara para os scores\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        # Calcula a softmax nos scores\n",
        "        attention = torch.softmax(scores                    , dim=-1)\n",
        "\n",
        "        # Multiplica a matriz de atenção pelo valor V\n",
        "        output = torch.matmul(attention, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Divide a última dimensão em (num_heads, d_k)\n",
        "        N, seq_len, d_model = x.size()\n",
        "        return x.view(N, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # Inverte a operação de split_heads\n",
        "        N, _, seq_len, _ = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(N, seq_len, self.num_heads * self.d_k)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        N = query.shape[0]\n",
        "        query_len, key_len, value_len = query.shape[1], key.shape[1], value.shape[1]\n",
        "\n",
        "        # Passa os valores de Q, K e V pela camada linear\n",
        "        Q = self.split_heads(self.W_q(query))\n",
        "        K = self.split_heads(self.W_k(key))\n",
        "        V = self.split_heads(self.W_v(value))\n",
        "\n",
        "        # Calcula a atenção\n",
        "        attention = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Combina as cabeças e aplica a camada linear final\n",
        "        output = self.combine_heads(attention)\n",
        "        output = self.W_o(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "multi_head_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "# (batch_size, sequence_length, d_model)\n",
        "query = torch.randn(batch_size, 50, d_model)\n",
        "key = torch.randn(batch_size, 50, d_model)\n",
        "value = torch.randn(batch_size, 50, d_model)\n",
        "\n",
        "output = multi_head_attn(query, key, value)\n",
        "\n",
        "print(output.shape)  # Output shape: (batch_size, sequence_length, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wLtA1etBOvDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac34299-1641-4313-bdbf-ad7c8ec57e73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 50, 512])\n"
          ]
        }
      ],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "\n",
        "d_model = 512\n",
        "d_ff = 2048\n",
        "ffn = FeedForward(d_model, d_ff)\n",
        "\n",
        "# (batch_size, sequence_length, d_model)\n",
        "input_tensor = torch.randn(32, 50, d_model)\n",
        "\n",
        "output = ffn(input_tensor)\n",
        "\n",
        "print(output.shape)  # Output shape: (batch_size, sequence_length, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MNXgUgTcOvDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2744efd1-9180-443f-baa9-cf59e679d6d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 50, 512])\n"
          ]
        }
      ],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.attention(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout(ffn_output))\n",
        "        return x\n",
        "\n",
        "\n",
        "encoder_layer = EncoderLayer(d_model, num_heads, d_ff)\n",
        "\n",
        "# (batch_size, sequence_length, d_model)\n",
        "input_tensor = torch.randn(32, 50, d_model)\n",
        "\n",
        "output = encoder_layer(input_tensor)\n",
        "\n",
        "print(output.shape)  # Output shape: (batch_size, sequence_length, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hyDToTNKOvDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05aae1ea-332c-4804-b66b-e20870b4564c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 100, 512])\n"
          ]
        }
      ],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, src_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Embedding + positional encoding + dropout\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Passa a entrada por cada camada do encoder\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "src_vocab_size = 1000\n",
        "num_layers = 6\n",
        "max_len = 100\n",
        "\n",
        "encoder = Encoder(src_vocab_size, d_model, num_heads, num_layers, d_ff, max_len)\n",
        "\n",
        "# (batch_size, sequence_length)\n",
        "input_seq = torch.randint(0, src_vocab_size, (32, 100))\n",
        "\n",
        "output = encoder(input_seq)\n",
        "\n",
        "print(output.shape)  # Output shape: (batch_size, sequence_length, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0okz8q6FOvDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddbc4f52-f998-48e4-bde2-8279951fcd0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 50, 512])\n"
          ]
        }
      ],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.cross_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask=None, trg_mask=None):\n",
        "        # Self-attention na sequência de destino\n",
        "        self_attn_output = self.self_attention(x, x, x, trg_mask)\n",
        "        x = self.norm1(x + self.dropout(self_attn_output))\n",
        "\n",
        "        # Cross-attention entre a saída do self-attention e a saída do encoder\n",
        "        cross_attn_output = self.cross_attention(x, enc_out, enc_out, src_mask)\n",
        "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
        "\n",
        "        # Feed-forward\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm3(x + self.dropout(ffn_output))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "decoder_layer = DecoderLayer(d_model, num_heads, d_ff)\n",
        "\n",
        "# (batch_size, sequence_length, d_model)\n",
        "input_tensor = torch.randn(32, 50, d_model)\n",
        "enc_out = torch.randn(32, 50, d_model)\n",
        "\n",
        "output = decoder_layer(input_tensor, enc_out)\n",
        "\n",
        "print(output.shape)  # Output shape: (batch_size, sequence_length, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Tkbm0A-vOvDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6284a221-684b-46a2-b154-e578438b4f60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 100, 1000])\n"
          ]
        }
      ],
      "source": [
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, trg_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(trg_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.fc_out = nn.Linear(d_model, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask=None, trg_mask=None):\n",
        "        # Embedding + positional encoding + dropout\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Passa a entrada por cada camada do decoder\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, src_mask, trg_mask)\n",
        "\n",
        "        out = self.fc_out(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "trg_vocab_size = 1000\n",
        "num_layers = 6\n",
        "max_len = 100\n",
        "\n",
        "decoder = Decoder(trg_vocab_size, d_model, num_heads, num_layers, d_ff, max_len)\n",
        "\n",
        "# (batch_size, sequence_length)\n",
        "trg_seq = torch.randint(0, trg_vocab_size, (32, 100))\n",
        "enc_out = torch.randn(32, 100, d_model)\n",
        "\n",
        "output = decoder(trg_seq, enc_out)\n",
        "\n",
        "print(output.shape)  # Output shape: (batch_size, sequence_length, trg_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZpT0C4wUOvDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54baadfd-70e5-4b32-bb04-82f53d277075"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 100, 1000])\n"
          ]
        }
      ],
      "source": [
        "# Transformer Completo\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, d_ff, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_heads, num_encoder_layers, d_ff, max_len, dropout)\n",
        "        self.decoder = Decoder(trg_vocab_size, d_model, num_heads, num_decoder_layers, d_ff, max_len, dropout)\n",
        "\n",
        "    def generate_mask(self, src, trg):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        trg_mask = (trg != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = trg.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "        trg_mask = trg_mask & nopeak_mask\n",
        "        return src_mask, trg_mask\n",
        "\n",
        "    def forward(self, src, trg, src_mask=None, trg_mask=None):\n",
        "        src_mask, trg_mask = self.generate_mask(src, trg)\n",
        "        enc_out = self.encoder(src, src_mask)\n",
        "        out = self.decoder(trg, enc_out, src_mask, trg_mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "src_vocab_size = 1000\n",
        "trg_vocab_size = 1000\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_encoder_layers = 6\n",
        "num_decoder_layers = 6\n",
        "d_ff = 2048\n",
        "max_len = 100\n",
        "\n",
        "transformer = Transformer(src_vocab_size, trg_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, d_ff, max_len)\n",
        "\n",
        "# (batch_size, sequence_length)\n",
        "src_seq = torch.randint(0, src_vocab_size, (batch_size, 100))\n",
        "trg_seq = torch.randint(0, trg_vocab_size, (batch_size, 100))\n",
        "\n",
        "output = transformer(src_seq, trg_seq)\n",
        "\n",
        "print(output.shape)  # Output shape: (batch_size, target_sequence_length, trg_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YgSXcsiSOvDw"
      },
      "outputs": [],
      "source": [
        "# Gerando as máscaras de forma separada\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "    return (seq != 0).unsqueeze(1).unsqueeze(2).type(torch.uint8)  # Cria uma máscara para posições de preenchimento\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = (1 - torch.triu(torch.ones(size, size), diagonal=1)).type(torch.uint8)\n",
        "    return mask  # Cria uma máscara triangular para impedir a atenção em tokens futuros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Zv7oHVhxOvDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "585db495-8a9f-4860-b997-94fd789d1164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[1, 1, 0, 1, 0]]]], dtype=torch.uint8)\n"
          ]
        }
      ],
      "source": [
        "seq = torch.tensor([[1, 2, 0, 4, 0]])\n",
        "padding_mask = create_padding_mask(seq)\n",
        "print(padding_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "IJzQS29xOvDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b90be080-18e1-44ec-ee33-48c6a3580e31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 0, 0, 0, 0],\n",
            "        [1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1]], dtype=torch.uint8)\n"
          ]
        }
      ],
      "source": [
        "look_ahead_mask = create_look_ahead_mask(5)\n",
        "print(look_ahead_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt8yclFKOvDw"
      },
      "source": [
        "## Exercícios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xheQ2HRoOvDx"
      },
      "source": [
        "### Exercício 1\n",
        "Implemente um módulo que utilize apenas o módulo Encoder para a classificação de texto em `num_classes` classes. Para a obtenção do vetor de embedding de toda a sequência que será enviado para a cabeça de classificação, faça um pooling de média através da dimensão de sequência."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nj-tOUVyOvDx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc94a93-3e85-4ac0-c6aa-134abd1f2e78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 5])\n"
          ]
        }
      ],
      "source": [
        "class TextClassifier(nn.Module):\n",
        "  def __init__(self, src_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, num_classes, dropout=0.1):\n",
        "      super().__init__()\n",
        "      self.embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "      self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "      self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "      self.fc = nn.Linear(d_model, num_classes)\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "      # Embedding + positional encoding + dropout\n",
        "      x = self.embedding(x)\n",
        "      x = self.positional_encoding(x)\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # Passa a entrada por cada camada do encoder\n",
        "      for layer in self.layers:\n",
        "          x = layer(x, mask)\n",
        "\n",
        "\n",
        "      x = x.mean(dim=1)\n",
        "      out = self.fc(x)\n",
        "\n",
        "      return out\n",
        "\n",
        "src_vocab_size = 1000\n",
        "num_layers = 6\n",
        "max_len = 100\n",
        "\n",
        "classifier = TextClassifier(src_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, num_classes=5)\n",
        "\n",
        "# (batch_size, sequence_length)\n",
        "input_seq = torch.randint(0, src_vocab_size, (32, 100))\n",
        "\n",
        "output = classifier(input_seq)\n",
        "\n",
        "print(output.shape)  # Output shape: (batch_size, sequence_length, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGRQYrbVOvDx"
      },
      "source": [
        "### Exercício 2\n",
        "Vamos implementar um modelo baseado em stack de decoders. Uma vez que não é necessário cross-attention, pois não há encoders, utilize o módulo `EncoderLayer`. O tamanho do vocabulário deverá ser de 50257, o tamanho dos embeddings de 768, 12 cabeças de atenção, 12 camadas, dimensão da camada feedforward de 3072 e tamanho máximo de sequência 1024. Em seguida, teste com valores aleatórios simulando uma sequência de tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "D7yxdMraOvDx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "068b00bd-9085-44e6-b332-71db63db20b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 50257])\n"
          ]
        }
      ],
      "source": [
        "class TextGenerator(nn.Module):\n",
        "  def __init__(self, src_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1):\n",
        "      super().__init__()\n",
        "      self.embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "      self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "      self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "      self.fc = nn.Linear(d_model, src_vocab_size)\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "      # Embedding + positional encoding + dropout\n",
        "      x = self.embedding(x)\n",
        "      x = self.positional_encoding(x)\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # Passa a entrada por cada camada do encoder\n",
        "      for layer in self.layers:\n",
        "          x = layer(x, mask)\n",
        "\n",
        "\n",
        "      x = x.mean(dim=1)\n",
        "      out = self.fc(x)\n",
        "\n",
        "      return out\n",
        "\n",
        "src_vocab_size = 50257\n",
        "d_model = 768\n",
        "num_heads = 12\n",
        "num_layers = 12\n",
        "d_ff = 3072\n",
        "max_len = 100\n",
        "max_len = 1024\n",
        "\n",
        "generator = TextGenerator(src_vocab_size, d_model, num_heads, num_layers, d_ff, max_len)\n",
        "\n",
        "# (batch_size, sequence_length)\n",
        "input_seq = torch.randint(0, src_vocab_size, (32, 100))\n",
        "\n",
        "output = generator(input_seq)\n",
        "\n",
        "print(output.shape)  # Output shape: (batch_size, sequence_length, d_model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}